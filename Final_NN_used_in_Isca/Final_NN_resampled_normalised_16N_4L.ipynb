{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca919c-f160-4197-86ac-4d0e9fcfc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import iris\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import tensorflow as tf\n",
    "from netCDF4 import Dataset\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, LeakyReLU, Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "import xarray as xr\n",
    "from keras.models import model_from_json\n",
    "from keras import backend\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c2b2f-043d-4a79-b82d-0a397aecc4dc",
   "metadata": {},
   "source": [
    "Import Training Data and initialise random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a3b48-c970-40a2-9279-585f79a17f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['2021_t42_std.nc', '2022_t42_std.nc']\n",
    "all_data = xr.open_mfdataset(files)\n",
    "# select data to train on which is the first year\n",
    "data = all_data.sel(time = '2021')\n",
    "data_from_isca = xr.open_dataset('era_land_t42.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c59276-b6ee-4c44-abe0-2a00a952044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679d739-5cec-4c15-9933-72f8f8ca2cc1",
   "metadata": {},
   "source": [
    "Function to normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8f4d0-75cd-454b-b78c-b8993b4f555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(datain):\n",
    "\n",
    "    dataout=(datain-np.min(datain))/(np.max(datain)-np.min(datain))\n",
    "\n",
    "    return dataout;"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5ab51e1-1ca3-455c-9cd2-7c8cd0b2be6b",
   "metadata": {},
   "source": [
    "Reshape Inputs for NN and store min and max for normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4a9b2-af68-4db6-8b5b-123e11a6a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will rejig the features one at a time and I'll normalise and concatenate them separately\n",
    "def rejigging(data, plotting = False, times = range(len(data.time.values))): # times is just for plotting\n",
    "    # Plots of each of the times\n",
    "    if plotting:\n",
    "        for i in times:\n",
    "            df = data.sel(time=str(data.time.values[i]))\n",
    "            plt.pcolormesh(df)\n",
    "            plt.title(f'{data.time.values[i]}')\n",
    "            \n",
    "    # Starting with t2m - Do the first step manually by selecting the first time:\n",
    "    df = data.sel(time=str(data.time.values[0]))\n",
    "    # Reshape to 1d vector\n",
    "    var = np.reshape(df.values,(64*128, 1))\n",
    "    # Loop through the remaining times and append to a single array\n",
    "    for i in range(1, len(data.time.values)):\n",
    "        df = data.sel(time=str(data.time.values[i]))\n",
    "        #reshape into 1d vector\n",
    "        var_i = np.reshape(df.values,(64*128, 1))\n",
    "        #concatenate the different times\n",
    "        var = np.append(var, var_i, axis = 0)\n",
    "    # Store min and max\n",
    "    min_var = np.min(var)\n",
    "    max_var = np.max(var)\n",
    "    \n",
    "    return var, min_var, max_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6a78e-2512-4519-82bf-e7c8bc1ac7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['u10', 'v10', 't2m', 'sdor', 'rh2'] \n",
    "# t2m_std is what we predict - not included here so it can be last in the order later\n",
    "\n",
    "feature_min = {} # to store training means in a dictionary\n",
    "feature_max = {} # to store training stds in a dictionary\n",
    "normalised_features = {} # to store normalised features for training in a dictionary\n",
    "\n",
    "for f in features:\n",
    "    df = data[f]\n",
    "    reshaped = rejigging(df) # reshapes\n",
    "\n",
    "    # stores mean and std for each feature\n",
    "    feature_min[f'{f}'] = reshaped[1]\n",
    "    feature_max[f'{f}'] = reshaped[2]\n",
    "\n",
    "    # normalises the feature\n",
    "    normalised_features[f'{f}'] = normalise(reshaped[0])\n",
    "\n",
    "df = data_from_isca.zsurf\n",
    "# Reshape to 1d vector\n",
    "zsurf = np.reshape(df.values,(64*128, 1))\n",
    "# Store mean and std\n",
    "min_zsurf = np.min(zsurf)\n",
    "max_zsurf = np.max(zsurf)\n",
    "# Add copies for the different time points to match other vars\n",
    "zsurf_i = zsurf\n",
    "for i in range(1, len(data.time.values)):\n",
    "    zsurf = np.append(zsurf, zsurf_i, axis = 0)\n",
    "# normalise\n",
    "normalised_zsurf = normalise(zsurf)\n",
    "\n",
    "# Add to the lists of features made for main dataset\n",
    "# stores mean and std for each feature\n",
    "feature_min['zsurf'] = min_zsurf\n",
    "feature_max['zsurf'] = max_zsurf\n",
    "normalised_features['zsurf'] = normalised_zsurf\n",
    "\n",
    "# Adding t2m_std last:\n",
    "f = 't2m_std'\n",
    "\n",
    "df = data[f]\n",
    "reshaped = rejigging(df) # reshapes df\n",
    "\n",
    "# stores mean and std for each feature\n",
    "feature_min[f'{f}'] = reshaped[1]\n",
    "feature_max[f'{f}'] = reshaped[2]\n",
    "\n",
    "# normalises the feature\n",
    "normalised_features[f'{f}'] = normalise(reshaped[0])\n",
    "\n",
    "#define list with new feature names\n",
    "features = list(normalised_features.keys())\n",
    "\n",
    "# append to one dataset\n",
    "big_data = np.append(normalised_features[features[0]], normalised_features[features[1]], axis = 1) # join 0th and 1st feature into big data\n",
    "for i in range(2, len(normalised_features)): # join remaining features\n",
    "    big_data=np.append(big_data, normalised_features[features[i]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981a104-7ee8-4480-9c15-15933d5852b8",
   "metadata": {},
   "source": [
    "Histogram of normalised t2m_std - very unbalanced dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe58872-2e61-480e-94a7-78146b24fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 12          # Default text size\n",
    "plt.rcParams['axes.titlesize'] = 16     # Title font size\n",
    "plt.rcParams['axes.labelsize'] = 13     # X and Y axis labels font size\n",
    "plt.rcParams['xtick.labelsize'] = 12    # X tick labels font size\n",
    "plt.rcParams['ytick.labelsize'] = 12    # Y tick labels font size\n",
    "plt.rcParams['legend.fontsize'] = 13    # Legend font size\n",
    "plt.rcParams['figure.titlesize'] = 18   # Figure title font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb561e2a-fede-4785-91d4-38a33fe3abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "plt.hist(normalised_features['t2m_std'], bins = 10)\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xlabel('Normalised t2m_std')\n",
    "#plt.savefig('/home/links/sr850/isca_results/NN/NN_Training_Plots/hist_raw_data_normalistedt2m_std.png',  bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f285c-1b8a-447e-b7f5-ca9ca3bb3db2",
   "metadata": {},
   "source": [
    "Store Feature Maxima and Minima for unnormalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d7d3c-898b-48fd-b561-24e8f8590569",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80d1e1-28b3-4073-ab94-183df729be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_max"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69a469de-030b-4b1c-9a79-17479ba746ee",
   "metadata": {},
   "source": [
    "Resample Dataset to oversample minority classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f551d36-2447-4625-8bc0-28e00f458b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "\n",
    "norm_t2m_std = big_data[:, -1] # selecting last feature which is t2m_std\n",
    "\n",
    "maybe = []\n",
    "keep = []\n",
    "for i in range(len(norm_t2m_std)):         #maybe I should add one for above 0.9...\n",
    "    if  norm_t2m_std[i] >= .6:             # This criterion is to keep a proportion of indices of samples where std is greater than 0.8\n",
    "        maybe.append(i)\n",
    "    else:\n",
    "        keep.append(i)\n",
    "\n",
    "n= 3000 # just explicitly specify number n to keep from each of the bands\n",
    "\n",
    "keep = keep + random.choices(maybe, k = n) # keep less of the higher values\n",
    "\n",
    "resampled_big_data = np.zeros((len(keep), np.shape(big_data)[1])) \n",
    "for j in range(np.shape(big_data)[1]): # loops through 0 to 7 for the 8 features\n",
    "    for w in range(len(keep)): # cycles through the indices you've selected to keep\n",
    "        resampled_big_data[w, j] = big_data[keep[w], j] # assigns data to keep from big_data to resampled_big_data\n",
    "        \n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.hist(resampled_big_data[:, 6], bins = 10)\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xlabel('Normalised t2m_std')\n",
    "#plt.savefig('/home/links/sr850/isca_results/NN/NN_Training_Plots/hist_resampled_data_normalistedt2m_std.png',  bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53868a15-769a-44c2-9a1a-79450379bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "plt.hist(resampled_big_data[:, 6], bins = 10)\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xlabel('Normalised t2m_std')\n",
    "#plt.savefig('/home/links/sr850/isca_results/NN/NN_Training_Plots/hist_resampled_data_normalistedt2m_std.png',  bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6bd01-2ae5-429a-aabd-e3940c8383d4",
   "metadata": {},
   "source": [
    "Reshape prediction to match Isca's Grid and Unnormalise and perform validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e94dc8-6fa1-4f2a-ac38-0fc4eff9306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "# Shuffle\n",
    "\n",
    "#shuffled_big_data   = np.random.permutation(big_data) # Changed to the resampled dataset\n",
    "shuffled_big_data   = np.random.permutation(resampled_big_data) # Changed to the resampled dataset\n",
    "\n",
    "# 7 input features \n",
    "n_inputs=6 \n",
    "\n",
    "# So having put all the data together to shuffle, we now separate it again into its inputs/features/x\n",
    "# and the outputs/target/y.\n",
    "\n",
    "x     = shuffled_big_data[:,        :n_inputs]  # all rows, columns 0 up to n_inputs - 1 (so here we get the 0th and 1st column)\n",
    "y     = shuffled_big_data[:,n_inputs:        ]\n",
    "\n",
    "# The the first n % of the data and use that for training, the remainder will be used for validation.\n",
    "\n",
    "nt            = x.shape[0]\n",
    "\n",
    "n = 0.8 # 80% for training\n",
    "\n",
    "n_train       = int(n*nt)\n",
    "\n",
    "trainx, testx = x[:n_train, :], x[n_train:, :]\n",
    "\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# START: THINGS YOU MAY WISH TO VARY\n",
    "# TIER 1: (more worthwhile looking at the impact of changin these)\n",
    "# Define the number of nodes in each layer\n",
    "\n",
    "n_nodes = 16\n",
    "\n",
    "# The total number of layers (yes you could have different number of nodes in each layer, but this is simpler).\n",
    "n_layers = 4\n",
    "epochs   = 20\n",
    "\n",
    "# TIER 2: (you could change these too, but maybe try the ones above first).\n",
    "# Define the leakiness of the ReLU\n",
    "leaky_alpha = 0.1\n",
    "# Define likelihood of a node being randomly ignored during training.\n",
    "drop_out_rate = 0.2\n",
    "\n",
    "initial_learning_rate = 1.0e-3\n",
    "batch_size            = 128\n",
    "\n",
    "# Later we use a learning rate scheduler than reduces the learning rate every n epochs.\n",
    "reduce_every_n        = 5\n",
    "# END: THINGS YOU MAY WISH TO VARY\n",
    "# Start of definition of the multi-layer perceptron (MLP) or fully-connected neural network.\n",
    "# Note how after x=Dense()(input), the syntax is that x=f(x)\n",
    "# This means that it adds things to x progressively.\n",
    "# Here n_input is 4, for avg_theta, avg_orog, std_orog and avg_lsm\n",
    "input = Input(shape=(n_inputs,))\n",
    "X    = Dense(n_nodes)(input)\n",
    "X     = LeakyReLU(alpha=leaky_alpha)(X)\n",
    "\n",
    "# These are the hidden layers, you could define several layers manually, or use a nice loop to do it for yo.\n",
    "for layer in np.arange(1,n_layers):\n",
    "    X     = Dense(n_nodes)(X)\n",
    "    X     = Dropout(drop_out_rate)(X)\n",
    "    X     = LeakyReLU(alpha=leaky_alpha)(X)\n",
    "\n",
    "# The final or output layer, this has dimension 1, for the single prediction of std_theta that we are trying to predict.\n",
    "\n",
    "X     = Dense(1, activation='relu')(X)\n",
    "\n",
    "# Define the whole model is terms of the inputs and the things accumulated in x\n",
    "\n",
    "model = Model(inputs=input, outputs=X)\n",
    "\n",
    "# Prints everything to screen.\n",
    "# Do have a look at the output shape at each stage and the number of parameters associated with each layer.\n",
    "# Also look at final \"total number of parameters\".\n",
    "# Confirm that you are happy how these numbers are related to the number of nodes and layers you have defined.\n",
    "model.summary()\n",
    "\n",
    "# Learning rate scheduler\n",
    "# reduce the learning rate (i.e. the size of the updates to the weights and biases)\n",
    "# to help you home in on thebest value.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=int(nt / batch_size)*reduce_every_n,\n",
    "                                                             decay_rate=0.5,staircase=True)\n",
    "\n",
    "# Use the Adam optmizer.\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# State the loss that you want to minimise, here mean squared error.\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mse', 'R2Score'])\n",
    "#model.compile(optimizer=opt, loss=my_mse_loss, metrics=['mse'])\n",
    "#model.compile(optimizer=opt, loss=my_m2e_loss, metrics=['mse'])\n",
    "#model.compile(optimizer=opt, loss=my_m4e_loss, metrics=['mse'])\n",
    "#model.compile(optimizer=opt, loss='msle', metrics=['mse', 'R2Score'])\n",
    "\n",
    "# Do the actual training.\n",
    "# This learns to predict trainy using trainx\n",
    "# and also calculates how well it does at predicting testy using testx, which was NOT used as part of the minimisations.\n",
    "history=model.fit(trainx, trainy, validation_data=(testx, testy), epochs=epochs, batch_size=batch_size, shuffle=True)\n",
    "# This cost function is calculated using batch_size number of samples at a time.\n",
    "# An update is done after every batch. It will therefore take n_train/batch_size training steps to get through all the data once.\n",
    "# This is called an \"epoch\". We then shuffle the data before doing the next epoch.\n",
    "\n",
    "# Get hold of the mse for the training and validation (witheld) data.\n",
    "# Ideally you want a loss MSE for both. But not a lower MSE for train than for val (that suggests over-fitting to the train data).\n",
    "mse_train = history.history['loss']\n",
    "mse_val   = history.history['val_loss']\n",
    "R2Score_train = history.history['R2Score']\n",
    "R2Score_val = history.history['val_R2Score']\n",
    "\n",
    "# and plot it\n",
    "epoch_vec=np.arange(1,epochs+1)\n",
    "plt.figure()\n",
    "plt.plot(epoch_vec,mse_train,'k-')\n",
    "plt.plot(epoch_vec,mse_val,'b--')\n",
    "plt.title('mse')\n",
    "plt.show()\n",
    "# this allows you to see whether things have plateaued off, or whether there is overfitting.\n",
    "\n",
    "my_string='SR_resampled_minusd2m'+str(n_layers)+'L_'+str(n_nodes)+'N'\n",
    "# Write this info out\n",
    "fileout=my_string+'_mse_train.txt'\n",
    "np.savetxt(fileout, np.ones((1,1))*mse_train, fmt='%10.7f')\n",
    "fileout=my_string+'_mse_valid.txt'\n",
    "np.savetxt(fileout, np.ones((1,1))*mse_val, fmt='%10.7f')\n",
    "# Write the model architecture to one file.\n",
    "model_json=model.to_json()\n",
    "fileout=my_string+'.json'\n",
    "with open(fileout, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    json_file.close()\n",
    "\n",
    "# Write all the trained weights and biases to another file.\n",
    "    fileout=my_string+'.weights.h5'\n",
    "    model.save_weights(fileout)\n",
    "    \n",
    "print('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8f5e5-a4ec-425d-9065-61d3121c4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(epoch_vec,mse_train,'g-', label = 'training data')\n",
    "plt.plot(epoch_vec,mse_val,'b--', label = 'test data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xticks(np.arange(0, 21, 5))\n",
    "plt.ylim(0.0045, 0.0095)\n",
    "plt.legend()\n",
    "plt.savefig(f'/home/links/sr850/isca_results/NN/NN_Training_Plots/mse_trainingcurve_resampled_mse_20ep_{str(n_nodes)+'_' + str(n_layers)}.png',  bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65638f-c386-4f43-ae98-6a42188971bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predy = model.predict(testx)\n",
    "plt.figure()\n",
    "plt.plot(testy,predy,'k+')\n",
    "plt.plot([0,1],[0,1],'r-')\n",
    "plt.xlabel('Normalised truth ')\n",
    "plt.ylabel('Normalised prediction')\n",
    "plt.title('Test Data')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e76f09-7467-4b40-924d-47d98b8f49bd",
   "metadata": {},
   "source": [
    "## Now Define and normalise validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482b1eb-2bbe-441e-8fec-3980cd6405be",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = all_data.sel(time = '2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb2045-3cfd-4a75-a91a-d59196b83228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_with_training(datain, training_min , training_max):\n",
    "    dataout=(datain-training_min)/(training_max-training_min)\n",
    "    return dataout;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e12aa-920f-4661-bae9-eb2e2d7bbe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will rejig the features WITHOUT NORMALISING\n",
    "def rejigging2(data, plotting = False, times = range(len(data.time.values))): # times is just for plotting\n",
    "    # Plots of each of the times\n",
    "    if plotting:\n",
    "        for i in times:\n",
    "            df = data.sel(time=str(data.time.values[i]))\n",
    "            plt.pcolormesh(df)\n",
    "            plt.title(f'{data.time.values[i]}')\n",
    "            \n",
    "    # Starting with t2m - Do the first step manually by selecting the first time:\n",
    "    df = data.sel(time=str(data.time.values[0]))\n",
    "    # Reshape to 1d vector\n",
    "    var = np.reshape(df.values,(64*128, 1))\n",
    "    # Loop through the remaining times and append to a single array\n",
    "    for i in range(1, len(data.time.values)):\n",
    "        df = data.sel(time=str(data.time.values[i]))\n",
    "        #reshape into 1d vector\n",
    "        var_i = np.reshape(df.values,(64*128, 1))\n",
    "        #concatenate the different times\n",
    "        var = np.append(var, var_i, axis = 0)\n",
    "    # Store min and max\n",
    "    min_var = np.min(var)\n",
    "    max_var = np.max(var)\n",
    "\n",
    "    return var, min_var, max_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bc7e4-b4d9-4684-a643-a3a5df69d273",
   "metadata": {},
   "source": [
    "Select validation data (ideally for two months because the plotting is set up to plot the first of two):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a35e5-b4d5-4b52-a516-bf7835d08370",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data = validation_data.sel(time = slice(validation_data.time.values[0],validation_data.time.values[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52b84f-e175-44b7-9224-d8a30cec675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['u10', 'v10', 't2m', 'sdor', 'rh2'] \n",
    "unseen_normalised_features = {}\n",
    "\n",
    "for f in features:\n",
    "    df = unseen_data[f]\n",
    "    reshaped = rejigging2(df)\n",
    "    df = reshaped[0] # reshapes df\n",
    "\n",
    "    # normalises the feature\n",
    "    unseen_normalised_features[f'{f}'] = normalise_with_training(df, training_min = feature_min[f'{f}'], training_max = feature_max[f'{f}'])\n",
    "\n",
    "df = data_from_isca.zsurf\n",
    "# Reshape to 1d vector\n",
    "zsurf = np.reshape(df.values,(64*128, 1))\n",
    "# Add copies for the different time points to match other vars\n",
    "zsurf_i = zsurf\n",
    "for i in range(1, len(unseen_data.time.values)):\n",
    "    zsurf = np.append(zsurf, zsurf_i, axis = 0)\n",
    "# normalise\n",
    "unseen_normalised_features['zsurf'] = normalise_with_training(zsurf, training_min = feature_min['zsurf'], training_max = feature_max['zsurf'])\n",
    "\n",
    "# Adding t2m_std last:\n",
    "f = 't2m_std'\n",
    "\n",
    "df = unseen_data[f]\n",
    "reshaped = rejigging2(df)\n",
    "df = reshaped[0] # reshapes df\n",
    "\n",
    "# normalises the feature\n",
    "unseen_normalised_features[f'{f}'] = normalise_with_training(df, training_min = feature_min[f'{f}'], training_max = feature_max[f'{f}'])\n",
    "\n",
    "features = list(unseen_normalised_features.keys())\n",
    "\n",
    "unseen_big_data = np.append(unseen_normalised_features[features[0]], unseen_normalised_features[features[1]], axis = 1) # join 0th and 1st feature into big data\n",
    "for i in range(2, len(unseen_normalised_features)): # join remaining features\n",
    "    unseen_big_data=np.append(unseen_big_data, unseen_normalised_features[features[i]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf24a6-a686-4e74-803c-4d11ce8b4a59",
   "metadata": {},
   "source": [
    "Prediction Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f774df-54fe-440e-b946-27f0ee2a3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_x     = unseen_big_data[:,        :n_inputs]  # all rows, columns 0 up to n_inputs - 1\n",
    "unseen_y     = unseen_big_data[:,n_inputs:        ]\n",
    "pred_unseen_y = model.predict(unseen_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804edba-24e5-45ad-8b7c-7454ea9351d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(unseen_y,pred_unseen_y,'k+')\n",
    "plt.plot([0,1],[0,1],'r-')\n",
    "plt.xlabel('Normalised truth (unseen data)')\n",
    "plt.ylabel('Normalised prediction (unseen data)')\n",
    "plt.title('Not Seen In Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4ffad-6da5-4566-b2d1-f24ac929b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_2d(data_at_specific_time): # this assumes you run validation for two times\n",
    "    t = data_at_specific_time[0:int(16384/2),:] # 16384 must be np.shape(pred_unseen_y)[0], since you run validation for two times\n",
    "    reshaped = np.reshape(t, (64, 128))\n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948901a4-1e80-42b0-84c0-d0652a76a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped = reshape_to_2d(pred_unseen_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2879f57-bc6b-4ae6-ab98-170c55854de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalise(normalised_data, training_min , training_max):\n",
    "    return ((training_max - training_min) *(normalised_data)) + training_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b458b-aa8b-435e-bd2c-4cede281d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truth_plot(ax): # this needs unseen_data\n",
    "    t2m_std_t0 = unseen_data.t2m_std.sel(time = unseen_data.time.values[0], method = 'nearest')\n",
    "    contours = ax.contourf(unseen_data.lon, unseen_data.lat, t2m_std_t0, levels = np.arange(0, 11, 1), extend = 'max')\n",
    "    plt.colorbar(contours)\n",
    "    ax.set_title('Truth')\n",
    "    return t2m_std_t0 ;\n",
    "def prediction_plot(ax): # this needs pred_unseen_y and feature_min and max t2m_std\n",
    "    unnormalised_prediction = unnormalise(pred_unseen_y, feature_min['t2m_std'], feature_max['t2m_std'])\n",
    "    reshaped = reshape_to_2d(unnormalised_prediction)\n",
    "    # this is normalised\n",
    "    contours = ax.contourf(unseen_data.lon, unseen_data.lat, reshaped, levels = np.arange(0, 11, 1), extend = 'max')\n",
    "    plt.colorbar(contours)\n",
    "    ax.set_title('Prediction')\n",
    "    return reshaped;\n",
    "def prediction_minus_truth(ax,  difference): # this also needs unseen_data to be defined\n",
    "    l = int(np.abs((difference)).values.max()) # takes largest absolute value from difference and rounds to integer to set cbar lims\n",
    "    print('colorbar lim should be', l)\n",
    "    l=5 # trying to pick one to standardise across them all, but printing just to check I haven't set it too low\n",
    "    diff = ax.contourf(unseen_data.lon, unseen_data.lat, difference, levels = np.arange(-l, l+.01, 0.1), extend = 'both', cmap = 'seismic')\n",
    "    plt.colorbar(diff)\n",
    "    ax.set_title('Prediction - Truth')\n",
    "    return diff ;\n",
    "def unnormalised_truth_vs_prediction(ax, y , y_pred):\n",
    "    ax.plot(y,y_pred,'k+')\n",
    "    ax.plot([0, 10],[0,10],'r-')\n",
    "    ax.set_xlabel('Truth')\n",
    "    ax.set_ylabel('Prediction')\n",
    "    ax.set_title('Truth VS Prediction (unseen data)')\n",
    "    return ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583d0ff-a88f-4e6f-8b09-79e83ea405c0",
   "metadata": {},
   "source": [
    "Validation Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e484964-2e91-444b-a933-276a6cd019a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 1, ncols = 4, figsize = (16, 3))\n",
    "truth = truth_plot(axs[0])\n",
    "prediction = prediction_plot(axs[1])\n",
    "prediction_minus_truth(axs[2], prediction - truth)\n",
    "unnormalised_truth_vs_prediction(axs[3], y = np.reshape(truth,(64*128, 1)), y_pred = unnormalise(pred_unseen_y, feature_min['t2m_std'], feature_max['t2m_std'])[0:64*128])\n",
    "for i in [0, 1, 2]:\n",
    "    axs[i].set_xlabel('Latitude')\n",
    "    axs[i].set_ylabel('Longitude')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(f'/home/links/sr850/isca_results/NN/NN_Training_Plots/validation_plot_resampled_mse_{str(n_nodes)+'_' + str(n_layers)}.png',  bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe29bdd-d3cd-48c9-b48c-6ceaa9685a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(unseen_x, unseen_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d0c33-98ba-4153-a73e-d918f333872e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
